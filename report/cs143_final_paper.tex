\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{morefloats}
\usepackage{verbatim}
\usepackage{wrapfig}
\usepackage{float}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
%\usepackage{times}
\usepackage{hyperref}
\hyphenpenalty=5000
\tolerance=1000
\setlength\parindent{0pt}
\begin{document}
\title{Network Simulation}
\author{Maxwell Horton, Alex Jose, Michael Lauria, Anish Agarwal, Johno Ikpeazu}


\maketitle
\noindent
\tableofcontents
\newpage
\section{Introduction}

\section{Simulation Architecture}

\subsection{Event-Driven Simulation}
An event-based simulation was used. All network objects (hosts, links, routers and flows) are children of the same parent class named EventGenerators. Every EventGenerator has its own priority queue that it can throw events on. The priority queue is ordered by the timestamp of the events. There is a global handler that checks the minimum timestamp of each network object in each loop and processes the event with the smallest overall timestamp in the simulation in the loop. Processing an event means giving that event to the destination specified in the event itself (the source and destination of events have to necessarily be adjacent in the network topology, or the source and destination must be the same). 

Provided below is a high level description of what happens in each simulation loop:
\begin{enumerate}
\item Global handler iterates over EventGenerators
\item EventGenerators report their internal event queues (\emph{events are deterministic and know when they should trigger})
\item Simulator collects set of most-immediate events and sends them to their respective destinations.
\item Generators that have been given events react and generate new events with a later timestamp that they push onto their respective priority queues
\end{enumerate}

\subsection*{Reflection on event-based simulation}
The event-driven simulation paradigm ended up working well for our group. While it may initially be less intuitive than a continuous simulation, and plotting was sometimes difficult (as data is typically only logged for times where interesting things are happening), an event-based simulation is particularly well-suited for simulating networks, and was fairly simple to implement.

\subsection{Links}

Links are bi-directional with each side of a link having the same buffer-size and capacity. Links keep adding packets to their buffer until it has reached its maximum buffer occupancy, at which point it drops all subsequent packets until its buffer occupancy has dropped below the maximum. This is a short description of the procedure the link utilizes to update its link size and capacity:

\begin{enumerate}
\item Link gets an event – finds the difference between the packet event and its own internal clock
\item Decreases buffer occupancy by the difference in time multiplied by capacity
\item If $buffer occupancy + packet size$ is smaller than max buffer occupancy, add packet to its buffer
\item Update queue delay, buffer size and internal clock (based on event time)
\item Add an event to its priority queue that essentially tells the handler to send the packet to the adjacent network object at the appropriate time
\item $(event time + propagation delay + queue delay)$
\end{enumerate}

\subsection*{Reflection on Links}

Links were relatively straightforward to write. A conceptual error we made at the start was that we gave a link a single buffer, which had packets added to it simultaneously from both sides of the link. We managed to rectify this error relatively quickly. Logging was particularly difficult for links as we had to estimate the instantaneous link rate for a link based only on the information we get when a link receives an event. This procedure has been described in the segment of the report dealing with plotting. 





\subsection{Routers}

Routers run a distributed bellman-ford algorithm to find the shortest path between any pair of hosts. Each router has a routing table that is uses to figure out which neighboring link it should send information down for each flow. The routing table is implemented as an unordered map with the key being the host ID and the value a path from that router to that host.
 
A path is a vector of 3-tuples. The elements on the 3-tuple are the link ID, the delay on that link, and the most recent time the router updated the delay for that specific link. So a path from router to a host gives the exact set of links that will be traversed to get to that host and the total delay to that host.
 
At specified time intervals, a router will broadcast its routing table to all adjacent routers. Before broadcasting its table, it will find the most recent link delay for every neighboring links and update each path it has with the correct delay and update time. The time a router waits between broadcasting its routing table is given by the following formula: 
% make this mathy!
$$waitTime = min(5.0, waitTime + 0.1)$$

When a router receives a routing table from a neighboring router, it first updates all the relevant link weights in every path that it has if the link update time in the neighbor router’s, routing table is more recent.

It then executes the following procedure to check whether it should update the shortest path it has to each host. For each host in neighbors routing table, it checks if two conditions are met:
\begin{enumerate}
\item (Link delay between two hosts ink delay between two hosts + Total delay on other router’s path to host) < current distance of router to host
\item The path between the specified host and the neighbors router and current router do not form a cycle
\end{enumerate}
If both conditions are met, then it updates its routing table with the neighboring router’s path to that host (and appending the link between the two routers to that path).


\subsection*{Reflection on routers}
 
The main challenge we faced with routers was that old routing tables were being bounced around the network. The reason for this was that routers were initially not checking to see how old the routing table information they were getting was. Hence a router did not have an accurate estimation for what the actual link delay on a link was. The change we made to rectify this was that each link in a path was a given a most recent update time attribute. This allowed us to easily enforce that each link delay in a path in a router was only updated if information it received was recent enough. An assumption made with routers is that there is no delay between them receiving a data packet and sending it down the correct link



\subsection{Host}
The host receives PacketEvents from the handler through its giveEvent() function.  This function is split into several cases that are handled by helper functions.  The most common case is that a PacketEvent is received by the host.  In this case, the host is either at the sending or the receiving end of the flow.  To determine this, the host has two maps: ``flows'' and ``recvd''.  The former associates flow ids with the Flow objects whose sending ends are rooted at the host.  The latter associates flow ids with a list of received packets.  Since flow ids must be globally unique, we can determine whether a the host is at the sending end or receiving end of the flow with which the packet is associated.  (We can also use the information on whether the packet is an ack to do this).
\\\\
When a data packet is received, if it is an ACK, the host performs the appropriate action using the ``recvd'' object.  If it is not an ACK, the host passes the event to the appropriate Flow in ``flows'' using the flow id field of the packet.  Non-data packets (i.e. SYNs and FINs) are handled separately.

\subsection{Flow}
The Flow object handles many of the basic operations generic to Vegas and Tahoe.  It contains methods for opening connections, sending packets / adjusting window sizes, etc.

\subsection{VegasFlow}
Our implementation of Vegas updates on a timer set to fire once every RTT.  To do so, a VegasUpdateEvent is generated when the Flow connection is opened (i.e. when the SYNACK is received).  When the VegasUpdateEvent reaches the Host on which the VegasFlow object lives, it is passed to the VegasFlow to be handled.  To handle the VegasUpdateEvent, the window size is adjusted according to:
$$ w(t + 1) = \begin{cases} w + 1, & \gamma < \alpha / minRTT \\ w, & alpha \le \gamma \le \beta \\ w - 1, & \beta < \gamma \end{cases}, \gamma = \dfrac{w}{minRTT} - \dfrac{w}{RTT} $$,
\\\\
where RTT is the most recent RTT.

\subsection{TahoeFlow}
The Tahoe flow resets to slow start on receipt of three duplicate acks, or on a timeout.  If a such a reset occurs, the time of the reset is stored.  Any potential timeouts that would be triggered by an event generated before this time are ignored (to prevent the algorithm from being bombarded by timeouts).  Similarly, we ignore the duplicity of acks for packets with timestamps from before our most recent reset (because otherwise, the algorithm would be bombarded by a barrage of resets).  During slow start, the window size increases by one with every ack until the ssthresh is met.  In congestion avoidance, window size increases by $1/w$ with every ack.

\subsection{Input}
1) Input

Network description files consist of plain-text listings of various network components and their initialization parameters. Flex and Bison were used to lex and parse the network description files. Strings and floating-point numbers can be used to provide arguments, and references to previously-made objects can be made as well. For example, the description file for test case 1 is as follows:
\begin{verbatim}
# Network spec for test case 1
# link stuff link_rate stuff side1 side2 name
link 524288 0.01 12500000 host1   router1 link0
link 524288 0.01 10000000 router1 router2 link1
link 524288 0.01 10000000 router1 router3 link2
link 524288 0.01 10000000 router2 router4 link3
link 524288 0.01 10000000 router3 router4 link4
link 524288 0.01 12500000 router4 host2   link5
# host host_link name
host link0 host1
host link5 host2
# router [list of hosts] [adjacent links] name
router [host1 host2] [link0 link1 link2] router1
router [host1 host2] [link1 link3]       router2
router [host1 host2] [link2 link4]       router3
router [host1 host2] [link3 link4 link5] router4
# flowtype name destination stuff origin window_size stat_time
vegas flow1 host2 160000000 host1 1 0.5
\end{verbatim}

\subsection*{Reflection on Input}

After we settled on a simple format for the network description files, writing the lexer and parser were not too difficult due to the power of flex and bison. There were some strange issues with lexed strings overwriting each other when being accessed in the parser, but all of this was handled by using a string buffer to avoid depending on flex successfully preserving the value of multiple strings.



\subsection{Output}

Network output is logged by each network object through calls to a logging library. Each call to the logger is ‘self-documenting’ in the sense that the calling network object can pass its own label to the logger such that the logger can appropriately tag the passed data for easier plotting at a later point in time.

Plotting is accomplished using Gnuplot-iostream, which allows for plotting with Gnuplot in C++ through simply writing data to the standard output stream.

\subsection*{Reflection on Output}
One significant annoyance with the output code stemmed from the event-based nature of our simulation - it’s simple to get logging data from network objects when they’re sending or receiving events, but getting them to log data when no events are being passed is relatively more difficult. This is not a major problem, as interesting things are happening primarily when events are being passed, but was still a slight problem.


\section{Analysis}


\subsection{Simulation Analysis}
\subsubsection{Test Case 0}
%%%
% TEST0_TAHOE IMAGES
%%%

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test0_Tahoe/buffer_occ.png}
\caption{Buffer occupancy follows the same pattern for the same reasons as Flow RTT.}
\label{fig:test0_tahoe_buffer_occ}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test0_Tahoe/flow_rate.png}
\caption{The flow rate quickly reaches the bound imposed by our link capacities, then stays at that level until the flow is complete. Downward spikes correspond to returns to slow start.}
\label{fig:test0_tahoe_flow_rate}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test0_Tahoe/flow_rtt.png}
\caption{The round trip time seen by the flow rises with the window size as more packets are introduced to the system.  Thus, queue delays increase.}
\label{fig:test0_tahoe_flow_rtt}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test0_Tahoe/link_rate.png}
\caption{We see that link rate reaches the link capacity and stays there until the flow is complete. Again, downward spikes occur due to returns to slow start.}
\label{fig:test0_tahoe_link_rate}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test0_Tahoe/packet_loss.png}
\caption{Some packets are lost towards the beginning of the test, but most packets get through as Tahoe adjusts. Triple duplicate acknowledgment packets account for most of the returns to slow start, which explains why there is so little packet lost in spite of returning to slow start.}
\label{fig:test0_tahoe_packet_loss}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test0_Tahoe/window_size.png}
\caption{Window sizes rise exponentially in slow start until they reach untenable heights. Tahoe returns the window size to one, and sets the slow start threshold to begin congestion avoidance at half of the largest window size seen before the return to slow start.}
\label{fig:test0_tahoe_windnow_size}
\end{figure}

\newpage

%%%
% TEST0_VEGAS IMAGES
%%%

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test0_Vegas/buffer_occ.png}
\caption{Buffer occupancy tops out at around 20000 bits.}
\label{fig:test0_vegas_buffer_occ}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test0_Vegas/flow_rate.png}
\caption{Vegas sees a smooth increase in flow rate to capacity with no disruptions.}
\label{fig:test0_vegas_flow_rate}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test0_Vegas/flow_rtt.png}
\caption{The round trip time seen by the flow is a bit noisy at first as the window size increases, but soon reaches a constant value of about 22ms.}
\label{fig:test0_vegas_flow_rtt}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test0_Vegas/link_rate.png}
\caption{The link rate quickly climaxes and remains steady at the link capacity.  }
\label{fig:test0_vegas_link_rate}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test0_Vegas/window_size.png}
\caption{The window size increases until the round trip time falls within the range specified by the tuning parameters, then remains constant at around 25 packets.}
\label{fig:test0_vegas_window_size}
\end{figure}

\newpage

\subsubsection{Test Case 1}
%%%
% TEST1_TAHOE IMAGES
%%%

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test1_Tahoe/buffer_occ.png}
\caption{We see that buffer occupancy increases as the window size approaches the limit of what the system can handle before packet loss becomes an issue. The switching happens because the routers are updating at five second intervals. Oscillatory behavior of the system has already been explained in the problem sets.}
\label{fig:test1_tahoe_buffer_occ}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test1_Tahoe/flow_rate.png}
\caption{The flow rate for this test is quite a bit more jittery. The spikes at the beginning are due to the inital setting of the slow start threshold. The first return to slow start did not achieve a low enough threshold, so the process repeated itself, resulting in two spikes.}
\label{fig:test1_tahoe_flow_rate}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test1_Tahoe/flow_rtt.png}
\caption{The round trip time of the flow roughly follows the pattern of window size, with disturbances at the router update times.}
\label{fig:test1_tahoe_flow_rtt}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test1_Tahoe/link_rate.png}
\caption{The link rate also follows this trend, reaching capacity when the window size is near optimal but dropping at returns to slow start.}
\label{fig:test1_tahoe_link_rate}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test1_Tahoe/packet_loss.png}
\caption{At the beginning, packet loss is relatively high, but once the congestion avoidance phase starts at a lower threshold it begins to decrease.}
\label{fig:test1_tahoe_packet_loss}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test1_Tahoe/window_size.png}
\caption{The window size drops four times. The first two peaks occur because of the high slow start threshold. Then the congestion avoidance phase begins earlier, allowing window size to rise gradually for much of the time before it becomes too high once again and returns to slow start.}
\label{fig:test1_tahoe_window_size}
\end{figure}

\newpage

%%%
% TEST1_VEGAS IMAGES
%%%

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test1_Vegas/buffer_occ.png}
\caption{The spikes in buffer occupancy occur due to the dumping of packets onto empty links after a routing table update switches the path chosen by router one.}
\label{fig:test1_vegas_buffer_occ}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test1_Vegas/flow_rate.png}
\caption{The flow rate is slow to reach the bound imposed by link capacities. This is because of vegas updates are less frequent due to greater round trip times. Vegas uses round trip times to schedule updates.}
\label{fig:test1_vegas_flow_rate}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test1_Vegas/flow_rtt.png}
\caption{Once the flow reaches capacity, the routers start oscillating as the top and bottom links fill up with packets. This explains the short deviations in the round trip time.}
\label{fig:test1_vegas_flow_rtt}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test1_Vegas/link_rate.png}
\caption{Links rates of links one and two operate at capacity in the time intervals between the routing table updates at router one.}
\label{fig:test1_vegas_link_rate}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test1_Vegas/window_size.png}
\caption{The same behavior as flow rate can be seen in the gradual increase in window size to around 100 packets.}
\label{fig:test1_vegas_window_size}
\end{figure}

\newpage


\subsubsection{Test Case 2}
%%%
% TEST2_TAHOE IMAGES
%%%

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test2_Tahoe/buffer_occ.png}
\caption{Buffer occupancy spikes on link 1 as Tahoe sets its slow start threshold.  The occupancy drops as the timeout occurs.  We can see the flow on link3 disrupt the characteristic rise and fall pattern as data transmission starts.  Link 2’s buffer occupancy never climbs, because there is no competition for its buffer.  Links 1 and 3 limit the flow through buffer 2, so that its queue doesn’t fill.}
\label{fig:test2_tahoe_buffer_occ}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test2_Tahoe/flow_rate.png}
\caption{Flow rate for flow1 follows its Tahoe pattern until flow 2 starts.  The, we reach a steady state, with two flows competing for link 1.  As the third flow starts, the first flow’s rate drops, because it’s competing with 2 other flows (and flows 2 and 3 are each only competing with 1).  In the end, flow 3 takes control of the third link, and uses most of the capacity while flow 1 waits for flow 3 to end.}
\label{fig:test2_tahoe_flow_rate}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test2_Tahoe/flow_rtt.png}
\caption{We can see spikes in RTT as each flow begins.  The spikes and drops in flows 1 and 2 match up, because the flows travel in the same direction on the link (meaning timeouts happen at the same time).  When the third flow starts, it has a low RTT.  As flow 2 ends, flow 3 and flow 1 have matching RTTs since only flow 3 is limiting flow 1’s transmission rate at this point.  The RTTs drop at the same time, as before.  The RTT for flow 1 is always larger, since flow3 has control of most of the link3.  }
\label{fig:test2_tahoe_flow_rtt}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test2_Tahoe/link_rate.png}
\caption{Link rate climbs on all three links, since flow 1 uses all three links.  In the steady state, link 2 is saturated as data flow 2 starts, and link 3 is saturated as data flow 3 starts.  Again, link 2’s link rate is lower since only 1 flow is every using it.}
\label{fig:test2_tahoe_link_rate}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test2_Tahoe/packet_loss.png}
\caption{Packet loss on link 1 increases, then packet loss on link 2 spikes as flow3 competes with flow 1 for control of the link.}
\label{fig:test2_tahoe_packet_loss}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test2_Tahoe/window_size.png}
\caption{All flows show the characteristic Tahoe shape for their window sizes.  The third flow has a larger window size since it outcompetes the first for link 3 (since its window sizes is larger, and Tahoe doesn’t permit sharing very well).}
\label{fig:test2_tahoe_window_size}
\end{figure}

\newpage

%%%
% TEST2_VEGAS IMAGES
%%%

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test2_Vegas/buffer_occ.png}
\caption{Link 2’s buffer occupancy has to strictly be zero as it has no additional data being streamed into it other than what comes to it from Link 1. Hence, the queue size is only built up in Link 1 and by the time the data packets arrive at Link 2, they are correctly spaced from each other in time.
Link 1’s buffer occupancy drops when Flow 3 is introduced as Flow 1 reacts by halving its window size. Hence Link 1 now receives less data at any point of time, when all three flows are present. Once Flow 2 stops, Flow 1 increases its window size slightly due to the lack of congestion on Link 1, but its window size is still less than when only Flow 1 was present. This reduced window size means less data is being sent down Link 1 at any point of time compared to when only Flow 1 was present, leading to a drop in buffer occupancy.
Link 3’s buffer occupancy remains high as it receives data packets from both Flow 1 and Flow 3.  }
\label{fig:test2_vegas_buffer_occ}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test2_Vegas/flow_rate.png}
\caption{Description of flow rate and window size are explained during the comparison of the simulation and analytical results. }
\label{fig:test2_vegas_flow_rate}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test2_Vegas/flow_rtt.png}
\caption{Flow RTT of Flow 1 is significantly higher than that of Flow 2 and 3 since it has to traverse an additional two links to get to its destination.  We see that when flow 2 begins sending data, there is an expected spike in Flow 1’s RTT due to an increase in queue delay in link 1. Similarly, when Flow 3 starts sending data, there is a further jump in Flow 1’s RTT due to an increase in delay in link 3. Flow 2 is unaffected, as it shares no links in common with flow 3.}
\label{fig:test2_vegas_flow_rtt}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test2_Vegas/link_rate.png}
\caption{Link rate of Link 2 gets halved of its capacity due to the bottleneck present at link 1. Since Flow 1 and Flow 2 are sharing Link 1, the queue delay at Link leads to less data sent to Link 2 at any point. Link rate of Link 3 remains at capacity as it gets an additional stream of data from Flow 3 along with the data it is receiving from Flow 1. Once Flow 2 stops, Link rate for Link 1 drops as well. The reason being that Flow 1 still has a smaller window size than it originally had when it was the only flow in the simulator, as it has to avoid congestion at Link 3 due to Flow 3. }
\label{fig:test2_vegas_link_rate}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/Test2_Vegas/window_size.png}
\caption{Description of flow rate and window size are explained during the comparison of the simulation and analytical results. }
\label{fig:test2_vegas_window_size}
\end{figure}

\clearpage


\subsection{Analytical Analysis}
\subsubsection{Test Case 2 - Vegas}

%insert network image for test case 2

The TCP Vegas algorithm we follow is:
\[ W \leftarrow \left\{
  \begin{array}{l l}
    W+1 & \quad \text{if }\frac{W}{\tau_{min}} - \frac{W}{\tau} < \frac{\alpha}{\tau_{min}}\\
    W-1 & \quad \text{if }\frac{W}{\tau_{min}} - \frac{W}{\tau} > \frac{\beta}{\tau_{min}}\\
    W & \quad \text{otherwise}\\
  \end{array} \right.\]

We begin by making the following simplifcation of steady state:

$$ \frac{W^*}{\tau_{min}} - \frac{W^*}{\tau^*} = \frac{\frac{\alpha + \beta}{2}}{\tau_{min}} $$

Where 
\begin{itemize}
\item $\alpha$ is $1.5$,
\item $\beta$ is $2.5$,
\item $W^*$ and $\tau^*$ are window sizes,
\item RTT is in equilibrium
\item $\tau_{min}$ is the min RTT experienced
\end{itemize}

The flow rate (traffic throughput), $x$, can be written as $$x=\frac{WP}{\tau}$$ where
\begin{itemize}
\item $W$ is the window size
\item $P$ is the DATA\_PKT\_SIZE
\item $\tau$ is the round-trip-time
\end{itemize}

At steady state, $x=C^*$ which refers to the minimum link capacity a data\_pkt experiences in the entire path from source to destination. We can think of $C^*$ as the bottleneck link capacity.

Hence in steady state, $$\frac{C^*}{P} = \frac{W^*}{\tau^*} $$
$$\implies \frac{W^*}{\tau_{min} - \frac{W^*}{\tau^*} = \frac{2}{\tau_{min}}}$$
$$\implies W^* = 2 + \frac{W^*}{\tau^*}(\tau_{min}) = 2 + \frac{C^*}{P}(\tau_{min}) $$

So for any flow, at steady state, 
\begin{itemize}
\item flow rate $= C^*$ where $C^*$ is the bottle-neck capacity.
\item $W^*=2 + \frac{C^*}{P}(\tau_{min})$
\end{itemize}


$$\tau_{min}=\sum_{l=1}^{l=num\_links} (prop\_delay + transmission\_delay)$$
There is no queue delay. The time taken to go from source to destination is different compared to destination to sources, as for src $\rightarrow$ dest, data\_packet\_size is 1kb; for dest $\rightarrow$ src, data\_packet\_size is 64kb.


$$\tau_{min_{flow\_1}} = [5*10 ms + 3*\frac{1024*8}{10^7} + 2*\frac{1024*8}{1.25*10^7}] + [5*10 ms + 3*\frac{64*8}{10^7} + 2*\frac{64*8}{1.25*10^7}] = 0.104 s$$ 

$$\tau_{min_{flow\_2}} = [30 ms + 1*\frac{1024*8}{10^7} + 2*\frac{1024*8}{1.25*10^7}] + [30 ms + 1*\frac{64*8}{10^7} + 2*\frac{64*8}{1.25*10^7}] = 0.0623 s$$ 

$$\tau_{min_{flow\_3}} = \tau_{min_{flow\_2}}$$

Flow 1 has to go through 3 links of capacity 10 Mbps and 2 links of capacity 12.5 Mbps.
Flow 2,3 has to go through 1 link of capacity 10 Mbps and 2 links of capacity 12.5 Mbps.


\subsubsection*{Phase 1}
Only flow 1 present. $C^*$ will be the link capacity of L1/L2/L3 which has capacity 10 mbps as all other links have link capacity 12.5 mbps. Hence 
Flow 1: $C^* = 10 mbps$
Flow 1: $W^* = 2 + \frac{C^*}{P}(\tau_{min_{link1}})=128.96 $

\subsection*{Phase 2}
Flow 1 and Flow 2 are present. The only change in the formulation of the state is that $C^*$ changes. Now the bottleneck link is L1 as both hosts must share the capacity of the link. Hence each host gets exactly half of link 1's capacity, leading to $$ C^* = \frac{capacity\_of\_link1}{2} = 0.5*10^7 bits $$

Flow 1: $C^*$ = Flow\_rate of link 2 = $0.5*10^7$ bits

Flow 1: $W^* = 2+\frac{C^*}{P}(\tau_{min_{link1}}) = 64.48$, half of phase 1

Flow 2: $C^*$ = Flow\_rate of flow 1 = $0.5*10^7$ bits

Flow 2: $W^* = 2+\frac{C^*}{P}(\tau_{min_{link2}}) = 38.0$


\subsection*{Phase 3}
Flows 1, 2, and 3 present. Now bottleneck link in flow2 remains link1 as it shares half of the link capacity with flow 1. Hence $C^*$ for flow2 is the same as in phase 2. Thus flow rate and $C^*$ are the same.
Flow\_rate of flow 1 = Flow\_rate of link 2 = $0.5*10^7$ bits
All flows: $C^* = 0.5*10^7$ bits

Flow 1 $W^* = 2+\frac{C^*}{P}(\tau_{min_{link1}}) = 64.48$, same as phase 2

Flow 2 $W^* = 2+\frac{C^*}{P}(\tau_{min_{link2}}) = 38.0$, same as phase 2

Flow 3 $W^* = 2+\frac{C^*}{P}(\tau_{min_{link2}}) = 38.0$, same as flow 2

Flow 3 shares link 3 with flow 1 in the same way flow 2 shares link 2 with flow 1. Hence $C^*$ for flow 3 is the same as that for flow 2 since $\tau_{min_{flow3}}=\tau_{min_{flow2}}$. Thus flow rate and $W^*$ the same as for flow 2.

Flow 1's bottle neck is either link1 or link 2, it shares each link's capacity equally with flow 2 and flow 3 respetively. Hence it's $C^*$ and $\tau_{min}$ is the same as in phase 2.


\subsection*{Phase 4}
Flow 1 and 3 present.
Now flow 2 has ended and flow 1 and 3 share link 3's capcity equally. This is an equivalent structure to phase 2 except we replacer link 1 with link 3 and flow 2 with flow 3. Hence flow 1's $C^*$ and $W^*$ same as they are in phase 2; flow 3's $C^*$ and $W^*$ are same as phase 2's value for flow 2.
All flows: $C^* = 0.5*10^7$ bits

Flow 1: $W^* = 2+\frac{C^*}{P}(\tau_{min_{link1}}) = 64.48$, same as phase 2

Flow 3: $W^* = 2+\frac{C^*}{P}(\tau_{min_{link2}}) = 38.0$, same as flow 2 in phase 2


\subsection{Comparison of Analytical and Simulation results}
%insert a figure
We reran the simulation with an increment such that the flows reach a steady state more quickly. We also increased the time when flow 2 and flow 3 started, which allowed each flow to reach their steady state. But none of these changes affected the capacity or flow rate at steady state for any flow.

As you can see from Figures \ref{fig:flow_rate_comparison} and \ref{fig:window_size_comparison}, our simulation follows analytical results almost exactly. The reason that the simulation has a slightly smaller window size is due to the small artificial delay that we add at the host when it sends a packet (which ensures that link buffers do not reorder packets due to having the same timestamp).

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/flow_rate_comparison.png}
\caption{Flow rate of spaced-out test case 2.}
\label{fig:flow_rate_comparison}
\end{figure}

\begin{figure}[!ht]
\centering \includegraphics[bb= 0 0 1300 250, scale=.35]{figures/window_size_comparison.png}
\caption{Window size of spaced-out test case 2.}
\label{fig:window_size_comparison}
\end{figure}

\section{Project Process}
\subsection{Succeses}
\subsection*{Github}

For version control on this project we ended up using GitHub, which (aside from some headache caused by beginner’s
mistakes) worked very well. Specifically, we used the “Git Flow” branching strategy for developing features and merging 
them with a variety of ‘base’ branches. Basically, Git Flow consists of two to three branches - one master and one 
develop branch, optionally with an extra ‘hotfix’ branch. The master branch contains the latest production release; 
develop contains the most recent functional build of development code; hotfix can be used in the case where broken 
code is put into production. The real usefulness of Git Flow comes from feature development - whenever a new feature is 
to be included in the project, a branch off of develop can be made (usually called ``feature/FEATURE\_NAME'') wherein 
the new feature will be developed. After the feature is added, it can be merged back into the develop branch. This allows for the convenient and easy addition of multiple features from multiple developers to a code base, and definitely helped us during the course of the project (despite failing to maintain strict Git Flow discipline at numerous points).


\subsection*{Smart pointers}

Another feature which ended up being helpful (after a lot of initial annoyance) was the use of smart pointers rather than regular pointers and references in our C++ code. Smart pointers, in the context of our project, are abstract data types meant to assist in managing pointers to dynamically-allocated memory. Without smart pointers, it is typically the job of the memory allocator to determine when the allocated memory should be freed, usually when the data contained in that memory is no longer used. Freeing memory at undesired times, or not freeing it at all, can be the source of some of the most irritating bugs in a somewhat low-level language like C++. Smart pointers are meant to help alleviate some of theses problems by defining a condition in which memory will be freed - typically in the case when nothing in code references that memory. That is, smart pointers keep track of the number of references to a particular place in memory, and when nothing in the code cares anymore, that memory is freed.


\subsection{Issues}

\subsection*{Tests}

One of the most-regretted decisions (or non-decision) we made during the course of the project was not to more aggressively create tests for the code. A huge amount of time has been spent in the last weeks of the project checking (and very likely re-checking) various aspects of the code that could be considered basic “sanity tests” - things that could easily be encapsulated as unit tests if we had only focused on that from the start. Overall, this probably would have been the single thing that would have saved us the most time in the later debugging stages of the project.

\subsection*{Better code review}

In addition to testing, a major part of the typical “large code project” process that the group didn’t spend much time on was code review. Frequently there was only one set of eyes on any given part of code until it became clear that code didn’t work, at which point someone would go back and try to fix it, often having to familiarize or refamiliarize themselves with chunks of code they’ve never seen before. Code review would not only have reduced bugs and improved code quality, but would mean more people were familiar with larger portions of the code, which probably would have further aided in debugging efforts. More consistent and more thorough code review would have saved us almost as much time as thorough tests.

\subsection*{Distribution of labor}

While group members more or less pulled their own weight, our initial distribution of labor was seriously imbalanced. For example, half of the group was working on a part of the project that, while important, only took an evening to finish. While the distribution of labor naturally re-balanced to accommodate available work, it might have been smart to formally redistribute tasks after more was known about how long each part would take. This would have had a variety of advantages - for example, if specific people formally ‘owned’ portions of the project, it would have been easier to know exactly who to consult when a bug with a specific portion of the code arose, rather than expecting everyone to spend a lot of time understanding all parts of the code in depth.



\appendix
\section{Instructions on Compiling}

Compile using the 'make' command from the CS\_143 directory. Run the simulation by typing './simulation'. 
The simulation will ask you to input the number of seconds it is to run for. For the first two test cases, 30 seconds should be sufficient. For the last test case, 70 seconds is recommended.
If you need extra clarification, please refer to the documentation provided with the code.


\end{document}
